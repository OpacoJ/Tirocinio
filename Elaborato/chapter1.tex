\chapter{Algoritmi sequenziali}
Prima di avviarci nella teoria del calcolo parallelo è doverosa un'introduzione al calcolo sequenziale e, nello specifico, al concetto di algoritmo.\\
La definizione informale di un algoritmo è un concetto generalmente noto: un algoritmo è una sequenza finita di operazioni elementari (univoche e non ambigue) che, date in esecuzione a un agente, manipola diversi valori in input per ottenerne degli altri in output. In altre parole un algoritmo definisce implicitamente una funzione da un dominio di definizione (input) a un codominio specifico (output) e tale che per ogni input appartenente al dominio esista sempre un output corrispondente. Detto ciò, se definiamo un algoritmo A chiameremo $\mathrm{f}_{A}$ la funzione che associa ad ogni \textit{x} del dominio la corrispondente uscita $\mathrm{f}_{A}(x)$. In questa definizione è racchiuso implicitamente il problema risolto dall'algoritmo. In questo caso si parla di calcolo sequenziale, poiché le operazioni che definiscono la procedura desiderata vengono eseguite una alla volta (in sequenza).\\
Formalmente questo significa che, dato un problema $\mathrm{f}: I \rightarrow S$, in cui \textit{I} rappresenta l'insieme delle varie istanze e \textit{S} l'insieme delle soluzioni, possiamo affermare che un algoritmo \textit{A} risolve tale problema se $\mathrm{f}_{A}(x) = \mathrm{f}(x)$ per ogni istanza $x \in I$.\\
È bene precisare che per ogni problema f possono esistere numerosi algoritmi che lo risolvono in maniera differente, e poiché l'utilizzo di un algoritmo comporta sempre l'utilizzo di un certo quantitativo di risorse (tempo di esecuzione, memoria, ecc.), il saper scegliere o costruire un algoritmo che le sappia gestire in maniera adeguata non è un particolare di poco conto. Un cattivo utilizzo delle risorse, nel peggiore dei casi, può rappresentare un vero e proprio ostacolo a livello di esecuzione e per questo motivo è importante trovare un modo per valutare la bontà di un algoritmo. Un metodo largamente utilizzato per ottenere questa valutazione è quello di adottare un modello di calcolo preciso e di tradurre l'algoritmo in modo da poter essere interpretato dallo stesso.\\
Poiché la nostra analisi è orientata principalmente allo studio della complessità sequenziale, ci concentreremo su un comodo modello di calcolo, specializzato in questo campo: la macchina di Turing.
\section{Macchina di Turing Deterministica}
La macchina di Turing (MdT) è un modello ideale di calcolatore dalle meccaniche intuitive e semplici, solitamente usato più per l'analisi computazionale che per l'implementazione di calcolatori reali. Vista la sua semplicità, rappresenta uno dei modelli più utilizzati per identificare e studiare il calcolo sequenziale.\\
Informalmente una MdT è composta da un insieme di stati interni, in cui si può trovare la macchina (si definisce anche uno "stato iniziale", in cui si trova all'inizio dell'esecuzione, e un insieme di stati finali), un nastro di lunghezza infinita suddiviso in celle e una testina posizionata su una di esse, capace di leggere, scrivere o cancellare caratteri sul nastro. La macchina analizza il nastro una cella alla volta e in base al carattere letto e allo stato interno corrente esegue una "mossa", così composta:
\begin{itemize}
\item{la macchina cambia il proprio stato interno;}
\item{la macchina esegue un'operazione di scrittura o muove la testina di una cella a destra o sinistra.}
\end{itemize}
Se la sequenza di mosse eseguite è finita diciamo che la macchina si arresta sull'input considerato e diciamo che tale input è accettato se lo stato raggiunto nell'ultima configurazione è finale.\\
Formalmente, invece, possiamo vedere la MdT come una sestupla:
\begin{center}
{M = < Q, $\sum$, $\mathrm{q}_{0}$, B, $\delta$, F >\\}
\end{center}
In particolare:
\begin{itemize}
\item{Q è l'insieme degli stati interni;}
\item{$\sum$ è l'alfabeto con cui vengono espressi i dati sul nastro;}
\item{$\mathrm{q}_{0} \in Q$ è lo stato iniziale;}
\item{$B \in \sum$ è un simbolo particolare, detto simbolo vuoto o blank;}
\item{$\delta$ è la funzione di transizione definita come $\delta$: $Q \times \sum \rightarrow  Q \times \sum \times$ \{-1,1\};}
\item{$F \subseteq Q$ è  l'insieme di stati finali.}
\end{itemize}
Per ogni $q \in Q$ e ogni $a \in \sum$, la funzione $\delta(q, a)$ definisce una tripla (p, b, l), dove p rappresenta il nuovo stato, b il carattere scritto nella cella corrente e l il movimento che esegue la testina, rispettivamente a destra se l = -1, a sinistra se l = +1.\\
Una configurazione particolare della macchina M è composta dallo stato della macchina, dal contenuto del nastro e dalla posizione della testina. Il tutto è esprimibile come una stringa: $\alpha$q$\beta$, con $\alpha \in \sum^*$, $q \in Q$, $\beta \in \sum^+$. In questo caso $\alpha$ rappresenta la stringa a sinistra della testina, q lo stato corrente di M, mentre $\beta$ una stringa collocata a destra della testina, seguita da infiniti caratteri blank. Notiamo che allo stato iniziale la configurazione diventa la stringa $\mathrm{q}_{0} \beta$. In questo contesto definiamo un'altra operazione binaria sull'insieme delle configurazioni C, l'operazione $\vdash_{M}$, tale che per ogni $C1, C2 \in C$, vale che $C1 \vdash_{M} C2$ se e solo se C1 raggiunge C2 in una mossa. Più precisamente, data la configurazione $\alpha$q$\beta \in C$ allo scatto di una transizione $\delta(q, b) = (p, b, l)$ (supponiamo che $\beta = b\beta', \beta' \in \sum^*, b \in \sum$ ) distinguiamo due casi:\\
\begin{itemize}
\item{se l = +1 allora:}
\[\alpha qb \beta' \vdash_{M} \left\{
  \begin{array}{lr}
    \alpha cp \beta' & : \beta' \neq \epsilon\\
    \alpha cpB & : \beta' = \epsilon
  \end{array}
\right.
\]
\item{se l = -1 e $\alpha \neq \epsilon$ allora, posto $\alpha = \alpha' a $, $\alpha' \in \sum^*, a \in \sum$:}
\[\alpha qb \beta' \vdash_{M} \left\{
  \begin{array}{lr}
    \alpha' pa & : c = B, \beta' = \epsilon\\
    \alpha pac\beta' & : altrimenti
  \end{array}
\right.
\]
\end{itemize}
Osserviamo che se $\delta(q,b)$ non è definito , oppure $l = -1$ e $\alpha = \epsilon$, allora non esiste una configurazione $C2 \in C$ tale che $\alpha q \beta \vdash_{M} C$. In questo caso diciamo che q e una configurazione di arresto per M. Senza perdita di generalità possiamo supporre che ogni configurazione accettante sia una configurazione di arresto.\\
Un insieme finito ${C}_{i}$ con \textit{i = {1,..,m}}, di configurazioni di M è una computazione di M su input $w \in \sum$, se ${C}_{0} = {C}_{0}(w)$, ${C}_{i-1} \vdash_{M} {C}_{i} \forall i = 1,2,..,m$ e ${C}_{m}$ è di arresto. Se ${C}_{m}$ è anche accettante, diciamo che M accetta l'input w. Viceversa, se M non termina, possiamo dire che l'input w genera una computazione definita da infinite configurazioni.\\
Inoltre, se M si arresta su ogni input $x \in \sum^*$, diciamo che M risolve il problema di decisione $<\sum^*, q>$ dove, $\forall x \in \sum^*$:
\[q(x) =\left\{
  \begin{array}{lr}
    1 & \ se \ M  \ accetta \ x \\
    0 & altrimenti
  \end{array}
\right.
\]\\
La MdT ha avuto un ruolo fondamentale nell'informatica teorica: poiché è matematicamente dimostrato che per qualsiasi modello di calcolo ragionevole esiste una macchina di Turing associata (tesi di Church-Turing), questo ha reso possibile definire uno standard nell'analisi computazionale degli algoritmi e una loro classificazione dal punto di vista risolutivo.
\section{Complessità sequenziale}
Quando analizziamo un algoritmo dobbiamo tenere in considerazione due caratteristiche: il fatto che risolva correttamente il problema che gli viene chiesto di risolvere (correttezza) e quante risorse impiega per essere eseguito (complessità). Le risorse utilizzate da un algoritmo sono essenzialmente il tempo di calcolo e la memoria utilizzata, che possiamo esprimere come funzioni a valori interi positivi. Nello specifico un algoritmo A su input \textit{x} viene rappresentata da una funzione ${T}_{A}(x)$ per il tempo e da ${M}_{A}(x)$ per lo spazio.\\
Cercare di dare una definizione formale di queste misure può risultare problematico, soprattutto perché la variabile \textit{x} può assumere tutti i valori appartenenti al dominio di $A$. Per questo motivo si cerca di raggruppare le varie istanze del problema a seconda della loro dimensione, definendo una funzione che associa a ogni ingresso un numero naturale che rappresenta la quantità di informazione contenuta. Per fare un esempio, la dimensione di un numero naturale \textit{n} è $\lfloor \log(n) \rfloor +1$, ovvero la sua lunghezza in codice binario, mentre un array di $n$ elementi ha dimensione $n$.\\
Presentiamo un esempio classico per capire come ottenere il tempo di calcolo: l'ordinamento di un array. Vogliamo ordinare un vettore $[x_1,..,x_n]$ di $n$ numeri in ordine crescente nel seguente modo:\\
\\
\textit{Per i = 1,..,n esegui iterativamente le seguenti operazioni\\
    - seleziona il minimo tra i valori $[x_i,..,x_n]$\\
    - scambialo con $x_i$}\\
\\
In questo caso l'algoritmo esegue sempre $n + (n-1) + .. + 1 = \frac{n(n-1)}{2} $ passi per ottenere un vettore ordinato di dimensione n.\\
Tuttavia in molti problemi classificare le varie istanze mediante la loro dimensione non è sufficiente: è possibile che a input diversi \textit{x $\neq$ y}, ma aventi la stessa dimensione ($|x| = |y|$), si possa ottenere ${T}_{A}(x) \neq {T}_{A}(x')$.\\
Per ovviare a questo problema si definiscono le funzioni in modo da ottenere una stima \textbf{assoluta} di quella che è la complessità di un algoritmo. Ad esempio tra le varie stime ottenibili per il tempo ${T}_{A}(x)$ vengono solitamente considerati i seguenti casi:
\begin{itemize}
\item{caso peggiore: ${T}_{A}^w: \mathbb{N} \rightarrow \mathbb{N}$, ${T}_{A}^w(n) = \max({T}_{A}(x)\ t.c.\ |x| = n)$};
\item{caso migliore: ${T}_{A}^b: \mathbb{N} \rightarrow \mathbb{N}$, ${T}_{A}^b(n) = \min({T}_{A}(x)\ t.c.\ |x| = n)$};
\item{caso medio: ${T}_{A}^a: \mathbb{N} \rightarrow \mathbb{R}$ con $I_n = $ numero di istanze $x \in I$ di dimensione n, ${T}_{A}^a(n) = \frac{\sum \nolimits_{|x| = n} {T}_{A}(x)}{I_n}$};
\end{itemize}
Prediligere una stima piuttosto che un'altra è una decisione puramente legata all'utilizzo dell'algoritmo. Dal punto di vista analitico, però, si tende a prediligere l'andamento nel caso peggiore, ovvero nel caso in cui l'input richiede il maggior numero di iterazioni per terminare, o nel caso medio, che analizza la computazione da un punto di vista statistico.\\
Nel modello della MdT, data una macchina M = <Q,$\sum$,$\mathrm{q}_{0}$,B,$\delta$,F> e un input definito da una stringa $w \in \sum^*$, definiamo con $T_M(w)$ come il massimo numero di mosse da effettuare per terminare l'esecuzione.\\ Ovviamente $T_M(w) = +\infty \leftrightarrow$ M non termina su $ w $.\\
Quindi possiamo definire una funzione $T_M(n)$ di stima del tempo di calcolo nel seguente modo:
\begin{center}
{$T_M(n) = \max(T_M(w) \ t.c.\ w \in \sum^* , |w| = n)$}
\end{center}
Inoltre, data $ f $ una funzione a valori reali positivi, diciamo che M esegue una computazione in tempo $f(n)$ se $T_M(n) \leq f(n) \ \forall n \in \mathbb{N}$.
\subsection{Analisi Asintotica della Complessità}
Per confrontare tra di loro algoritmi che risolvono lo stesso problema, così da scegliere il migliore disponibile, si ricorre a criteri di valutazione più specifici. Quello più utilizzato è il calcolo computazionale asintotico (solitamente riferito al caso medio o peggiore di un algoritmo).\\
Sostanzialmente si tratta di valutare la complessità di un algoritmo su input avente un entrata di dimensioni molto grandi, ovvero ponendo $n \rightarrow +\infty$. Questo rende possibile non solo ottenere un buon metodo di confronto, ma anche classificare i vari algoritmi rispetto a questa stima.\\
Nonostante questo meccanismo tenda a vacillare in caso di problemi relativamente piccoli, rilevare anche una piccola differenza nell'ordine di grandezza della complessità di due procedure può determinare enormi differenze in termini di prestazioni.\\
Proviamo, per esempio, a confrontare diversi algoritmi che risolvono lo stesso problema aventi complessità asintoticamente pari a: $n, \ n\log(n), \ n^2, \ 2^n$. Supponendo che ogni operazione venga eseguita in $1\mu s$ l'esecuzione richiederà un tempo di esecuzione pari a\footnote{Nella tabella s= secondi, h = ore, c = secoli.}:
\\
\\
\label{my-label}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
			& $n=10$     & $n=100$       	& $n=1000$      & $n=10000$  & $n=100000$  \\   \hline
$n$   		& $10\mu s$  & $0.1 ms$      	&  $1ms$      	& $10s$  	 & $0.1s$      \\	\hline
$n\log(n)$  & $23\mu s$  & $460.5 \mu s$    &  $6.9 ms$     & $92.1 ms$  & $1.15 s$    \\	\hline
$n^2$ 		& $0.1ms$    & $10ms$        	&  $1s$         & $100s$     & $2.7h$      \\	\hline
$2^n$		& $1ms$      & $ 10^{14} c$     &  $\infty$     & $\infty$   & $\infty$    \\   \hline
\end{tabular}
\\ 
\\ 
\\ 
Notiamo che per problemi avente complessità lineare ($n \ o \ n\log(n)$) la computazione avvenga in tempo relativamente buono per problemi anche di grandi dimensioni. Al contrario algoritmi aventi complessità $n^k \ con \ k \geq 2 $ risultano convenienti solo per problemi contenuti, mentre andando su complessità esponenziali il problema esplode anche sulle piccole dimensioni. 
\subsection{Efficienza degli Algoritmi}
Mediante valutazioni precedenti si possono classificare la gran parte degli algoritmi realizzabili, distinguendoli mediante la loro complessità asintotica. Introduciamo così il concetto di \textbf{efficienza}: un algoritmo si dice efficiente quando la sua complessità è di \textbf{ordine polinomiale}, ovvero che vale $\mathcal{O}(n^k)$ con $k \geq 1$; un algoritmo si dice invece inefficiente quando la sua complessità è di \textbf{ordine superpolinomiale} (per esempio, $\mathcal{O}(n!)$ o $\mathcal{O}(k^n)$ con $k \geq 2$).\\
Va puntualizzato che non sempre è possibile creare un algoritmo efficiente (soprattutto nella MdT che abbiamo descritto). L'insieme dei problemi che è possibile risolvere a complessità polinomiale è detta classe P, nelle MdT deterministiche, e NP, nelle MdT non deterministiche. Non tratteremo nel dettaglio questi concetti, soprattutto perché sono ancora oggetto di dibattito\footnote{Il problema P = NP? legato ai problemi NP-completi è uno dei problemi irrisolti più famosi della matematica odierna.}, ma è bene specificare che esistono strumenti per stabilire se un problema è risolvibile o no in modo efficiente.\\
Un ultimo concetto da tener presente nella complessità algoritmi è il principio di \textbf{ottimalità}: dato un problema risolto con complessità pari a $f(n)$, diciamo che l'algoritmo che lo risolve è \textbf{ottimo} se qualunque altro algoritmo che lo risolve ha complessità $\mathcal{O}(f(n))$.
\chapter{Esempi di Problemi Risolti}
Di seguito verranno presentati alcuni esempi di algoritmi sequenziali, molti dei quali verranno implementati a livello di codice alla fine dell'elaborato.
\section{Ordinamento}
L'ordinamento di un insieme di oggetti confrontabili è uno degli problemi tipici del calcolo. Ne esistono di diversi tipi, il che lo rende un esempio perfetto per confrontare le varie alternative.\\
Prendiamo ad esempio due algoritmi piuttosto comuni in questo campo: quicksort e mergesort. In pseudocodice saranno:
\begin{verbatim}
    Quicksort(L = [a1,...,an])
        if(n <= 1) then
            return L
        else
            scegli un elemento "p" in L
	        calcola la lista "left" di elementi minori di p
	        calcola la lista "right" di elementi maggiori di p
	        left = Quicksort(left)
	        right = Quicksort(right)
	        return left : [p] : right


    Mergesort(L = [a1,..,an])
        begin
        if(n <= 1) then
            return L
        else
            dividi la lista L a metà, creando le liste "left" e "right" 
            left = Mergesort(left)
            right = Mergesort(right)
            return merge(left, right)
        end
        
    merge(left, right)
        begin
        if(left = []) return right
        else if(right = []) return left
        else
            h1 = head(left)
            h2 = head(right)
            if (h1 < h2) then
                t = tail(left)
                return [h1] : merge(t, right)
            else
                t = tail(right)
                return [h2] : merge(left, t)
        end
\end{verbatim}
In questo caso, ad esempio i vari casi di esecuzione cambiano, anche a seconda del caso in analisi:
\begin{itemize}
\item{caso migliore:  $\mathcal {O}(n \log {}n)$ per quicksort e  $\mathcal {O}(n \log {}n)$ per mergesort;}
\item{caso peggiore:  $\mathcal {O}(n^2)$ per quicksort e  $\mathcal {O}(n \log {}n)$ per mergesort;}
\item{caso medio:  $\mathcal {O}(n \log {}n)$ per quicksort e  $\mathcal {O}(n \log {}n)$ per mergesort.}
\end{itemize}
\section{Algebra Lineare}
Facciamo degli esempi riguardanti le operazioni con matrici:
\begin{itemize}
\item{prodotto tra matrici;}
\item{calcolo del determinante;}
\item{inversione di una matrice.}
\end{itemize}
\newpage
\noindent Nel primo caso l'algoritmo per calcolare il prodotto tra due matrici $A$ e $B$ è il seguente:
\begin{verbatim}
    ProdottoMatrici(A : matrice MxN, B : matrice NxP)
        begin
            C = matrice di zeri MxP
            for i = 1,..,M do
                for j = 1,..,P do
                    for k = 1,..,N do
                        C[i][j] = C[i,j] + A[i][k] + B[k][j]
                    done
                done
            done
            return C
        end
\end{verbatim}
In caso di prodotto di una matrice $MxN$ con una $NxP$ l'algoritmo avrà complessità asintotica  $\mathcal {O}(N*M*P)$; nel caso peggiore questo si traduce in una complessità $\mathcal {O}(n^3)$.\\
Nel caso del calcolo del determinante di una matrice quadrata $A$ di dimensione $n$, solitamente viene sfruttato il metodo di Laplace, mediante il seguente algoritmo ricorsivo:
\[det(A) =\left\{
  \begin{array}{lr}
    a_{1,1} & \ se \ n  \ = \ 1 \\
    \sum\limits_{i=1}^n (-1)^{i+k} a_{i,k} det(A_{i,k}) & \ altrimenti
  \end{array}
\right.
\]\\
in cui, preso un qualsiasi $i \in [1,..,n]$, $a_{i,k}$ è l'elemento $(i,k)$ della matrice $A$ e $A_{i,k}$ è il minore ottenuto eliminando da $A$ la \textit{i-esima} riga e la \textit{k-esima} colonna.\\
In questo caso la complessità è evidentemente inefficiente: si tratta di calcolare ricorsivamente per ogni determinante di ordine $n$ altri $n$ determinanti di ordine $n-1$. In altre parole possiede una complessità di $\mathcal {O}(n!)$. Si tratta ovviamente di un algoritmo altamente sconveniente da utilizzare, anche per problemi di piccole dimensioni. In questi casi è consigliabile cambiare strategia: una possibile alternativa è rappresentata dal \textit{metodo di eliminazione di Gauss}. In sostanza si converte una matrice quadrata in una matrice triangolare (cioè formata da zeri al di sotto/sopra della sua diagonale) e calcolarne il determinante (operazione che si riduce a moltiplicare gli elementi sulla diagonale).
\newpage
\noindent La procedura consiste in tre passaggi:
\begin{enumerate}
\item{\textit{Se la prima riga ha il primo elemento nullo, essa viene scambiata con una riga che ha il primo elemento non nullo. Se tutte le righe hanno il primo elemento nullo, vai al punto 3.\\}}
\item{\textit{Per ogni riga $A_{i}$ con primo elemento non nullo, eccetto la prima, moltiplica la prima riga per un coefficiente scelto in maniera tale che la somma tra la prima riga e $A_{i}$ abbia il primo elemento nullo (quindi coefficiente = $-\frac{a_{i1}}{a_{11}}$. Sostituisci $A_{i}$ con la somma appena ricavata.\\}}
\item{\textit{Ripeti il punto 1 sulla sottomatrice ottenuta cancellando la prima riga e la prima colonna.}}
\\
\end{enumerate}
In questo caso cambiare algoritmo offre un guadagno considerevole: da una complessità fattoriale si passa ad una polinomiale, pari a $\mathcal{O}(n^2)$.\\
L'ultimo esempio dell'algebra lineare che affronteremo è quello della matrice inversa. Il metodo più semplice è quel calcolo dei cofattori, in cui data una matrice A invertibile ($det(A) \neq 0$) allora:
\begin{center}
{$A^{-1} = \frac{1}{det(A)}(cof(A))^T$\\}
\end{center}
In questo caso $cof(A)$ è la matrice dei cofattori o dei complementi algebrici di A, $a_{11},.., a_{nn}$, ottenuti nel seguente modo:
\begin{center}
{$a_{ij} = (-1)^{i+j} det(A_{ij})$ \\}
\end{center}
Con $i,j\rightarrow[1,..,n]$, $A_{ij}$ = A priva della $i-esima$ riga e della $j-esima$ colonna.\\
In questo caso la complessità dipende esclusivamente dall'algoritmo usato per ottenere il determinante: poiché in questo caso è un'operazione eseguita $n^2$ volte, l'efficienza/inefficienza si conserva.
\section{Grafi}
Un grafo è una struttura matematica composta da un insieme di elementi detti nodi che si possono collegare tra loro mediante oggetti archi. Formalmente un grafo $G$ è una coppia di elementi $<N, E>$ in cui $N$ è l'insieme dei nodi, ed $E$ l'insieme degli archi; ogni arco è definito come $e \ = \ (n1,n2)$ in cui $n1,n2 \in N$ sono i nodi collegati. Definiamo inoltre "cammino" da un nodo $n_1$ a un nodo $n_m$ una sequenza di archi $(n_1,n_2),(n_2,n_3),..,(n_{m-1}, n_m)$.
\newpage
\noindent Esistono numerose proprietà e operazioni sui grafi, ma ci concentreremo su alcuni classici problemi riguardanti i grafi:
\begin{itemize}
\item{grafo connesso;}
\item{cammino minimo.}
\end{itemize}
Molti di questi problemi sfrutteranno le leggi dell'algebra lineare, ovvero costruendo una matrice di adiacenza così definita: si crea una matrice quadrata di dimensione pari alla cardinalità dell'insieme dei nodi in cui ogni elemento $m_{ij}$ della matrice rappresenta un possibile collegamento tra i nodi $n_i$ e $n_j$; se l'arco $(n_i,n_j)$ o $(n_j,n_i)$ appartengono all'insieme degli archi del grafo, allora $m_{ij} = 1$, altrimenti $m_{ij} = 0$. Poiché semplicità studieremo i grafi non orientati (ovvero i grafi tali che ogni arco $i-j$ rappresenta anche un arco $j-i$).\\
Analizziamo ora i vari problemi nel dettaglio. Nel primo caso diremo che un grafo $G = <N, E>$ è connesso se e solo se per ogni coppia di nodi $n_i, n_j \in N$ esiste un cammino che li unisce. Il metodo più semplice per verificare questa proprietà è quello sfruttare la matrice di adiacenza $M$ relativa $G$. \\
L'algoritmo è il seguente:
\begin{verbatim}
    n = numero di nodi
    M = matrice di adiacenza
    for k = 1,..,n do
        for j = 1,..,n do
            for i = 1,..,n do
                if Mij = 0 then Mij = Mik && Ckj
            done
        done
    done
\end{verbatim}
\newpage
\noindent Se al termine dell'algoritmo la matrice contiene degli zeri il grafo non è connesso.\\
Si tratta di tre cicli innestati, proporzionali alla dimensione del grafo, dunque la complessità di calcolo equivale a $\mathcal{O}(n^3)$.\\
Il secondo caso che affronteremo è la ricerca di un cammino di costo minimo tra due nodi raggiungibili. Esistono molti algoritmi che risolvono questo problema, ma ci concentreremo su quello più utilizzato e conosciuto: l'algoritmo di Dijkstra. Si tratta di un algoritmo appartenete alla categoria degli \textit{algoritmi greedy}, ovvero quelli che, partendo da una soluzione parziale, la estendono fino a che non è più possibile. A ogni iterazione l'algoritmo sceglie l'estensione giudicata più conveniente e ripete l'esecuzione.\\
L'istanza del problema è data da un grafo $G = <N,E>$, una sorgente $s \in N$ e una funzione $w: E \rightarrow \mathbb{Q}$ che definisce un costo (o "peso")\footnote{Si considererà $w(e) \geq 0 \ \forall e \in E$} di ogni arco presente nel grafo. Definiamo inoltre $L(n)$ la lista dei nodi raggiungibili da $n$ con un arco $e \in E$. Dati questi elementi l'algoritmo calcolerà i cammini di costo minimo che collegano $s$ con ogni nodo $n \in N$.\\
Prima di eseguire l'algoritmo si definiscono tre insiemi: l'insieme S dei nodi per cui la soluzione è già stata calcolata, l'insieme D dei nodi non presenti in S e raggiungibili mediante un arco dai nodi presenti in S, e un insieme contenente tutti gli altri nodi rimanenti. A ogni nodo $n$ appartenente a D sono inoltre associate due procedure $C(n)$ e $P(n)$: il primo indica il costo del cammino minimo da $s$ a $n$ trovato finora, il secondo indica l'ultimo nodo prima di $n$ in tale cammino.
\newpage
La procedura è la seguente:
\begin{verbatim}
    Dijkstra(N, E, w, s)
        begin
            D = {s}
            for n in N do
                C(n) = infinity
                P(n) = NULL
            done
            C(s) = 0
            while D \= {} do
                begin
                    seleziona il nodo v in D avente C(v) minore
                    cancella n da D
                    for u in L(u) do
                        if C(v) + w(v,u) < C(u) then
                            if C(u) = infinity then
                                aggiungi u a D
                            end if
                            C(u) = C(v) + w(v,u)
                            P(u) = v
                        end if
                    done
                end
            return C, P
        end
\end{verbatim}
