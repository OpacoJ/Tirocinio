\chapter*{Prefazione}

La prima domanda da porsi per comprendere al meglio le ragioni dietro il lavoro in oggetto è la seguente: come è possibile ottimizzare al meglio le risorse della macchina? La seconda è conseguentemente: esclusa la multiprogrammazione, limitata all'esecuzione concorrente dei processi, che alternative ho a disposizione, se il problema riguarda un programma solo?\\
Come da titolo una possibile alternativa è rappresentata dal parallelismo, una vera e propria "parallelizzazione" del lavoro mediante l'ausilio di "lavoratori" (i processori) che eseguono ciascuno una parte del lavoro (i threads) concorrentemente.\\
Naturalmente il passaggio dal sequenziale (ovvero programmazione priva di parallelismo) al parallelo non è decisamente indolore. Oltre a una pianificazione decisamente più complessa del programma da eseguire, subentrano numerose problematiche legate alla coordinazione dei processori o alla dimensione del problema.\\ 
Va dunque detto questa tesi non ha come finalità il rendere obsoleto il concetto di programmazione sequenziale: al contrario verranno mostrati casi in cui la parallelizzazione non solo è sconveniente, ma addirittura meno performante della sua controparte.\\
Lo scopo ultimo del lavoro è piuttosto mostrare in primo luogo come molti problemi, spesso anche relativamente semplici nella loro comprensione e implementazione sequenziali, risultino lenti e poco efficienti (soprattuto dal punto di vista di tempo di esecuzione) al crescere della loro dimensione. In questi casi il parallelismo trova un suo perché, arrivando a migliorare l'esecuzione ottimizzando le risorse della macchina.\\
Dimostrato questo concetto, subentra il problema di come implementare queste caratteristiche in un programma. Per questa seconda parte ci serviremo di un linguaggio di programmazione di comodo utilizzo, Haskell, un linguaggio funzionale avente a disposizione astrazioni e librerie che consentono di parallelizzare problemi generici già a livello di codice, a vantaggio del programmatore.