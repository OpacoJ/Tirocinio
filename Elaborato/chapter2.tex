\chapter{Gli Algoritmi Paralleli}
Nel calcolo sequenziale procedure e funzioni sono eseguite da un singolo processore. Di conseguenza la velocità di esecuzione di un algoritmo è legata sia alla complessità che lo definisce sia alla velocità di calcolo del processore che lo esegue. In casi particolari, come i sistemi real time, questa caratteristica assume un ruolo critico e le limitazioni del calcolo sequenziale diventano evidenti.\\
Una possibile soluzione a questo problema è rappresentata dalla suddivisione del lavoro in sezioni da eseguire contemporaneamente da più processori, nel tentativo di ottenere una significativa riduzione del tempo di calcolo proporzionale all'aumento di risorse utilizzate. I sistemi di questo tipo vengono detti \textit{paralleli}.\\
Come si può facilmente intuire l'esecuzione parallela necessita di operazioni aggiuntive per coordinare i vari processori, aggiunta che può diventare problematica se mal gestita (approfondiremo questo concetto in un secondo momento). La comunicazione tra i processori può avvenire in due modalità, a seconda della categoria del modello in uso: distinguiamo due famiglie di sistemi paralleli, ovvero i modelli a memoria condivisa e i modelli a memoria distribuita. Nel primo caso i processori utilizzano contemporaneamente una memoria globale, che viene usata anche per casi di coordinazione (i processori non comunicano direttamente tra loro). Nei sistemi distribuiti, invece, ogni processore ha una propria memoria locale, inaccessibile agli altri, e la comunicazione avviene in maniera diretta. La maggior parte dei sistemi in uso sfruttano processori multicore a memoria condivisa, dunque ci concentreremo principalmente su questa categoria di modelli.\\
\newpage
\noindent Il passaggio da sequenziale a parallelo offre sicuramente la possibilità di ottenere un guadagno considerevole in termini di esecuzione, ma al tempo stesso apre la porta a nuove sfide: 
\begin{itemize}
\item{sono necessarie ulteriori analisi del problema, in particolare se e come è possibile creare una procedura capace di parallelizzarlo (le parti che è possibile eseguire parallelamente);}
\item{esistono problemi impossibili da parallelizzare;}
\item{è necessaria una coordinazione tra i vari processori;}
\item{la programmazione parallela spesso risulta più complicata di quella sequenziale.}
\end{itemize}
Nel corso di questo capitolo analizzeremo l'approccio a queste problematiche, servendoci di un modello di calcolo ad analizzare il parallelismo.
\section{La macchina PRAM}
Il modello che andremo ad analizzare è essenzialmente un'evoluzione di un particolare modello sequenziale. Nel capitolo precendente ci siamo serviti della Macchina di Turing come modello di calcolo ideale per analizzare le caratteristiche degli algoritmi sequenziali. Un altro esempio di modello sequenziale è la macchina RAM (Random Access Memory), formata essenzialmente da un singolo processore, capace di eseguire un set di operazioni elementari, e una memoria potenzialmente infinita di celle.\\
Questo modello nel calcolo parallelo definisce la base di quello che è il modello di calcolo che useremo: la macchina PRAM, un modello composto da un insieme di $p$ processori RAM e una memoria globale.\\
Come abbiamo già affermato l'esecuzione dei processori avviene in maniera indipendente: l'esecuzione di un'attività su un processore non influisce sull'esecuzione di uno dei suoi "compagni". Lo scambio di dati, invece, avviene mediante la memoria globale, a cui i processori accedono in tempo $\mathcal {O}(1)$ (Random Access).\\
\newpage
\noindent L'attività di un singolo processore continua a procedere in modo sequenziale: a ogni ciclo di clock un processore può decidere se effettuare operazioni sui dati che possiede o se effettuare operazioni di lettura e scrittura sulla memoria condivisa. In caso di esecuzione parallela ad un singolo ciclo di clock corrispondo molteplici istruzioni eseguite in contemporanea: in questo modo si ottiene il parallelismo desiderato.\\
Si distinguono i seguenti tipi di PRAM:
\begin{itemize}
\item{EREW (Exclusive Read Exclusive Write): l'accesso contemporaneo a memoria condivisa non è consentito;}
\item{CREW (Concurrent Read Exclusive Write): l'accesso contemporaneo a memoria condivisa è consentito solo in lettura;}
\item{CRCW (Concurrent Read Concurrent Write): l'accesso contemporaneo a memoria condivisa è consentito in lettura e in scrittura.}
\end{itemize}
Nel seguito illustreremo in particolare modelli EREW e CREW, sia dal punto di vista implementativo che da quello analitico.\\
In generale un'istruzione parallela ha questa forma:
\begin{verbatim}
for (i from 1 to NumProcessori) do
           operazioneParallela
\end{verbatim}
in cui \texttt{NumProcessori} rappresenta il numero di processori che vogliamo sfruttare, mentre \texttt{operazioneParallela} rappresenta una sequenza di istruzioni da eseguire esclusivamente su un singolo processore.\\
Indicheremo con $\mathrm{T}_{A}(n, p)$ il tempo di calcolo legato ad un algoritmo parallelo \textit{A} eseguito su $p$ processori su un generico input di dimensioni $n$. Notiamo che il caso base, in cui \textit{p = 1} coincide con il valore relativo al caso dell'esecuzione sequenziale, trattandosi di esecuzione monoprocessore. Questo elemento servirà per stimare la bontà dell'esecuzione parallela.\\
Poiché lo scopo della parallelizzazione è quello di ottenere un compromesso conveniente tra prestazioni e costo delle risorse aggiuntive ci serviremo di due misure particolari: lo \textit{Speedup} e l'\textit{Efficienza}, che indicheremo rispettivamente con $\mathrm{S}_{A}(n, p)$ e $\mathrm{E}_{A}(n, p)$.\\
Nel dettaglio:
\begin{center}
$\displaystyle \mathrm{S}_{A}(n, p) = \frac{\mathrm{T}_{A}(n, 1)}{\mathrm{T}_{A}(n, p)}$
\end{center}
\begin{center}
$\displaystyle \mathrm{E}_{A}(n, p) = \frac{\mathrm{S}_{A}(p)}{p} = \frac{\mathrm{T}_{A}(n, 1)}{p\mathrm{T}_{A}(n, p)}$
\end{center}
\newpage
\noindent Notiamo che lo Speedup rappresenta il guadagno in termini di tempo di esecuzione relativa all'utilizzo di \textit{p} processori e l'efficienza risulta essere il rapporto tra il tempo dell'algoritmo sequenziale e il tempo totale consumato dai processori, come se fossero usati sequenzialmente. In altri termini lo Speedup misura la riduzione del tempo di calcolo, mentre l'efficienza definisce quanto l'algoritmo in uso sfrutta il parallelismo della macchina.\\
Dato un algoritmo $A$ che lavora con $p$ processori con una data efficienza $E$, è in
generale possibile estendere l'algoritmo a lavorare con un numero inferiore di processori senza che l'efficienza diminuisca significativamente:
\begin{center}
Se $k \geq 1 \ \rightarrow \ E_{A}(n, \frac{p}{k}) \geq E_{A}(n,p)$
\end{center}
Dato un algoritmo $A$ che lavora con $p$ processori, basta infatti costruire un algoritmo che utilizza $\frac{p}{k}$ processori. Ad ogni nuovo processore si fa corrispondere un blocco di k vecchi processori: ogni nuovo processore usa al più k passi per emulare il singolo passo parallelo dei k processori corrispondenti. Quindi vale che:
\begin{center}
$\displaystyle T_{A}(n,\frac{p}{k}) \leq k T_{A}(n,p)$
\end{center}
Osservando che:
\begin{center}
$\displaystyle E_{A}(n, \frac{p}{k}) = \frac{T_{A}(n,1)}{\frac{p}{k} T_{A}(n, \frac{p}{k})} \geq \frac{T_{A}(n,1)}{p T_{A}(n, \frac{p}{k})} = E_{A}(n,p)$
\end{center}
concludiamo che l'efficienza non diminuisce diminuendo i processori. In particolare, poiché $E_{A}(n,p) \leq E_{A}(n,1) = 1$, l'efficienza non può superare 1, ovvero il caso ideale, in cui l'algoritmo utilizza tutti i processori per tutta l'esecuzione.\\
Ciò che limita lo speedup e di conseguenza l'efficienza degli algoritmi rappresenta l'\textit{Overhead} di esecuzione, ovvero calcoli non necessari alla sola risoluzione del problema, e può essere definito come:
\begin{center}
$O_{A}(n) = p T_{A}(n,p) - T_{A}(n,1)$
\end{center}
Il tempo speso in Overhead è determinato da diversi fattori: momenti di idle, eventi di comunicazione, ecc.
\newpage
\section{Limiti della Parallelizzazione}
Le leggi inerenti allo speedup e all'efficienza specificano implicitamente che non sempre l'aumento di più processori garantisce un guadagno in termini di prestazioni. Proviamo a dimostrarlo con un semplice esempio: immaginiamo di voler definire un algoritmo $A$ che calcoli la somma di $n$ numeri disposti in un array, dividendo quest'ultimo in $p$ parti pari al numero di processori utilizzati. I processori sommeranno i valori contenuti nei loro array e restituiranno il risultato una volta finita l'esecuzione, dopodiché un singolo processore raggrupperà e sommerà tra loro tutti i risultati ottenuti. Il tempo di esecuzione sarà pari a:
\[\left\{
  \begin{array}{lr}
   T_{A}(n,1) \simeq n\\
   T_{A}(n,p) \simeq + \frac{n}{p} \log_2(p)
  \end{array}
\right.
\]
Il conseguente overhead risulta quindi:
\begin{center}
$O_{A}(n) \simeq p T_{A}(n,p) - T_{A}(n,1) = p(\frac{n}{p} + \log_2(p)) - n = p \log_2(p)$
\end{center} 
Deduciamo quindi che l'overhead aumenta al crescere del numero di processori (in questo caso con una crescita pari a $\mathcal{O}(p \log_2(p))$).\\
L'unico modo per limitare calcoli di overhead è cercare di ottenere uno speedup prossimo al caso ideale (ovvero avente crescita lineare). Per ottenere questo risultato occorre un'ulteriore analisi relativa al tempo di esecuzione. Dividiamo il tempo $T_A(n,1)$ in due parti: chiameremo $T_{As}$ il tempo inerente alle operazioni eseguite esclusivamente in ambiente sequenziale e $T_{Ap}$ quello inerente alle operazioni eseguite in parallelo; ovviamente $T_A(n,1) = T_{As}(n) + T_{Ap}(n)$. Ora possiamo ottenere $T_A(n,p) \ p \geq 2$ in funzione di $T_A(n,1)$. Notiamo che, mentre la parte sequenziale rimane fissa al valore $T_{As}$, la componente parallela decresce in proporzione a $p$. Quindi otterremo:
\begin{center}
$\displaystyle T_{A}(n,p) = T_{As}(n) + \frac{T_{Ap}(n)}{p}$
\end{center}
Se chiamiamo $\alpha$ la componente sequenziale $T_{As}$ otteniamo che:
\begin{center}
$\displaystyle S_{A}(n,p) = \frac{1}{\displaystyle \alpha + \frac{1-\alpha}{p}}$
\end{center}
\newpage
\noindent La formula che abbiamo ottenuto non è altro che un caso particolare della \textit{legge di Amdahl}, che afferma un concetto coerente con quello che abbiamo cercato di esprimere:\\ \\
\textit{"Il miglioramento che si può ottenere su una certa parte del sistema è limitato dalla frazione di tempo in cui tale attività ha luogo"}\\ \\
Intuitivamente è ragionevole che la miglioria legata all'aumento di processori perda il suo effetto nel momento in cui venga utilizzato esclusivamente il calcolo sequenziale. Infatti, poiché il miglioramento della componente parallela si riduce ad aumentare il numero di processori in uso, se facciamo tendere $p$ a $\infty$ allora $S_A(n,p) \rightarrow \frac{1}{\alpha}$. In altre parole lo speedup ottenuto dalla parallelizzazione di qualsiasi algoritmo è sempre limitato dalla sua componente sequenziale.\\
Da questi risultati possiamo anche affermare che:
\begin{center}
$\displaystyle E_{A}(n,p) = \frac{S_{A}(n,p)}{p} = \frac{1}{\alpha + (1 - \alpha)p} = \frac{1}{\alpha (p-1) + 1}$
\end{center}
\begin{center}
$\displaystyle O_A(n,p) = (p - 1) \alpha$
\end{center}
\chapter{Esempi di Problemi Paralleli Risolti}
Di seguito verranno presentati i casi di problemi presentati precedentemente nel caso parallelo.
\section{Ordinamento}
Abbiamo già mostrato che l'algoritmo Mergesort si compone essenzialmente di due fasi di calcolo: nel caso base di array di due elementi l'ordinamento è banale; in caso contrario l'algoritmo divide un array a metà, lo ordina ricorsivamente e effettua l'operazione di merge sui due risultati. L'operazione di merging, essendo efficiente (complessità $\mathcal{O}(n)$), può essere lasciata invariata ed eseguita sequenzialmente: avendo a disposizione $p$ processori, il parallelismo si limiterà semplicemente ad assegnarne $\frac{p}{2}$ per le due parti in cui abbiamo diviso l'array. In altre parole il parallelismo si limita a fornire sotto-array ordinati, pronti per la fase merging.\\
Avendo a disposizione abbastanza processori questa procedura consente di ridurre la complessità di calcolo per la fase non merging a $\mathcal{O}(1)$, ottenendo una complessità totale di $\mathcal{O}(n)$. Ovviamente il vincolo per ottenere questo risultato è quello di avere un numero di processori pari a $n/2$.
\newpage
\noindent
\section{Algebra Lineare}
Riproponiamo di seguito i problemi che trattiamo nell'ambito dell'algebra lineare:
\begin{itemize}
\item{prodotto tra matrici;}
\item{calcolo del determinante;}
\item{inversione di una matrice.}
\end{itemize}
Il primo esempio presentato per le operazioni tra matrici è quello del prodotto. In questo caso osserviamo che dal punto di vista computazionale ogni elemento che compone la matrice risultante può essere calcolato in modo del tutto indipendente dagli altri: calcolare l'elemento in posizione $(i,j)$ necessita esclusivamente dell'$i-esima$ riga della prima matrice e della $j-esima$ colonna della seconda. Possiamo quindi scrivere l'algoritmo parallelo in questo modo:
\begin{verbatim}
    ProdottoMatriciParallelo(A : matrice MxN, B : matrice NxP)
        begin
            C = matrice di zeri MxP
            for i = 1,..,M do
                for j = 1,..,P do
                    Esegui in parallelo la seguente operazione:
                        for k = 1,..,N do
                            C[i][j] = C[i,j] + A[i][k] + B[k][j]
                        done
                done
            done
            return C
        end
\end{verbatim}
Avendo a disposizione $M*P$ processori otteniamo una complessità pari a $\mathcal{O}(N)$.\\
Il calcolo del determinante, come abbiamo visto, può essere implementato in modo inefficiente (metodo di Laplace) o efficiente (metodo di riduzione di Gauss). La parallelizzazione del primo caso è semplice: va effettuata sul calcolo dei vari determinanti per ogni minore ottenuto scorrendo una riga/colonna della matrice. Data una matrice $A$ di dimensioni $N$x$N$ possiamo quindi scrivere l'algoritmo in questo modo:
\begin{enumerate}
\item{\textit{Caso base: se N = 1 allora restituisci l'unico elemento presente in matrice.}}
\item{\textit{Se N > 1, scegli una riga o una colonna i tale che $i \in [1,..N]$ e calcola in parallelo i determinanti dei minori $A_{i,j}$ per ogni $j \in [1,..,N]$ e moltiplicali per $(-1)^{i+j}$.}}
\item{\textit{Somma i risultati ottenuti.}}
\end{enumerate}
Anche in questo caso il parallelismo garantisce un buon guadagno di prestazioni. Concentriamoci ora sul caso efficiente: ricordiamo che si tratta di eseguire sottrazioni iterativamente tra le varie righe (prese due a due) per ottenere una matrice triangolare, dopodiché si moltiplicano i valori sulla diagonale. In questo caso l'unico modo per introdurre parallelismo è quello di eseguire in parallelo il prodotto finale: ricavare la matrice triangolare non può essere fatto in parallelo. Di fatto il calcolo va eseguito in larga parte in modo sequenziale, dunque può risultare poco conveniente introdurre risorse aggiuntive.\\
Vedremo nel capitolo successivo come questo si ripercuote concretamente in un'esecuzione parallela.\\
L'ultimo caso, l'operazione di inversione di matrici, è per certi versi legato al calcolo del determinante, ma fortunatamente qui possiamo introdurre un altro livello di parallelismo. Invertire una matrice $N$x$N$ significa calcolare $N^2$ determinanti, ognuno in maniera indipendente dagli altri. Possiamo quindi scrivere, presa una matrice $A$ di dimensioni $N$x$N$:
\begin{enumerate}
\item{\textit{Calcola la matrice dei cofattori, in cui ogni cofattore è calcolato in parallelo.}}
\item{\textit{Moltiplica ogni elemento per $\frac{1}{det(A)}$.}}
\item{\textit{La matrice $A^{-1}$ è la matrice dei cofattori traslata.}}
\end{enumerate}
\section{Grafi}
La connettività di un grafo viene risolta mediante l'utilizzo delle matrici di adiacenza. Vista l'efficienza ottenuta nell'algebra lineare lasceremo inalterato l'algoritmo: ci limiteremo rendere parallele le operazioni interne all'esecuzione.\\
In primo luogo la creazione della matrice di adiacenza avviene nello stesso modo: presa una matrice di zeri, vengono analizzati gli archi del grafo e viene sostituito l'elemento associato nella matrice con un 1. La potenza $n-esima$ della matrice di adiacenza definirà le parti connesse del grafo.\\
Il prodotto matriciale per la potenza viene eseguito in parallelo, ottenendo così il parallelismo desiderato.